# Qwen3 ASR Workspace 部署说明

本文档描述基于 Docker 运行 Qwen3 ASR API 服务、并在本机提供 Web 前端的部署流程，与《2026年02月04日 - Qwen3 ASR配置过程记录》中的约定一致（容器名、挂载路径、端口及 `--shm-size` 等）。适用于 Windows 宿主机 + Docker 容器内 API 的典型用法。

## Contents

- [配置与约定](#配置与约定)
- [前置条件](#前置条件)
- [部署步骤](#部署步骤)
  - [首次部署：创建并启动容器](#首次部署创建并启动容器)
  - [已有容器：启动并进入](#已有容器启动并进入)
  - [容器内启动 API 服务](#容器内启动-api-服务)
  - [宿主机启动前端服务](#宿主机启动前端服务)
  - [访问与验证](#访问与验证)
- [关闭与退出](#关闭与退出)
- [离线模型部署](#离线模型部署)
- [故障排除](#故障排除)

---

## 配置与约定

| 配置项 | 默认值 | 说明 |
|--------|--------|------|
| 容器名称 | `qwen3-asr-container` | 可自定义，与下文命令一致即可 |
| 宿主机 API 端口 | `8000` | 访问地址：`http://localhost:8000` |
| 容器内 API 端口 | `80` | 映射 `-p 8000:80` |
| 宿主机工作目录 | `D:\Code\GitHub\Qwen3-ASR\workspace` | 含 `api_server.py`、`client.html` |
| 容器内挂载路径 | `/data/shared/Qwen3-ASR` | 与宿主机工作目录绑定 |
| 前端服务端口 | `8080` | 页面地址：`http://localhost:8080/client.html` |
| Docker 镜像 | `qwenllm/qwen3-asr:latest` | 官方镜像 |

---

## 前置条件

- **Docker Desktop** 已安装并启用（推荐 WSL 2 后端）。
- 宿主机工作目录存在且包含 `api_server.py`、`client.html`。
- （可选）GPU 版本需 NVIDIA 驱动及 Docker 内启用 NVIDIA Container Runtime。

---

## 部署步骤

### 首次部署：创建并启动容器

在 PowerShell 或 CMD 中执行。将 `D:\Code\GitHub\Qwen3-ASR\workspace` 替换为实际工作目录路径。

**CPU 版本：**

```cmd
docker run --name qwen3-asr-container -p 8000:80 -it ^
  -v D:\Code\GitHub\Qwen3-ASR\workspace:/data/shared/Qwen3-ASR ^
  --shm-size=4gb qwenllm/qwen3-asr:latest
```

**GPU 版本（需 NVIDIA GPU）：**

```cmd
docker run --gpus all --name qwen3-asr-container -p 8000:80 -it ^
  -v D:\Code\GitHub\Qwen3-ASR\workspace:/data/shared/Qwen3-ASR ^
  --shm-size=4gb qwenllm/qwen3-asr:latest
```

未拉取镜像时，可先执行：

```cmd
docker pull qwenllm/qwen3-asr:latest
```

执行后将进入容器内的 shell。

### 已有容器：启动并进入

若容器已存在（例如名称 `qwen3-asr-container`）：

```cmd
docker start qwen3-asr-container
docker exec -it qwen3-asr-container bash
```

### 容器内启动 API 服务

在容器内执行：

```bash
cd /data/shared/Qwen3-ASR
python api_server.py
```

服务监听容器内 80 端口，宿主机通过 `http://localhost:8000` 访问。保持该终端不关闭。

- 模型拉取：`api_server.py` 默认使用 `HF_ENDPOINT=https://hf-mirror.com`。若仍出现连接 Hugging Face 超时，请参考 [离线模型部署](#离线模型部署)。

### 宿主机启动前端服务

另开一个 PowerShell 或 CMD 窗口，在宿主机工作目录下启动 HTTP 服务（避免通过 `file://` 打开页面导致麦克风权限异常）：

```cmd
cd D:\Code\GitHub\Qwen3-ASR\workspace
python -m http.server 8080
```

保持该终端不关闭。若 8080 已被占用，可改用其他端口（如 8081），并相应修改浏览器访问地址。

### 访问与验证

1. **健康检查**：浏览器访问 `http://localhost:8000/`，应返回 `{"status":"running"}`。
2. **打开前端**：访问 `http://localhost:8080/client.html`。
3. **功能说明**：
   - **上传音频**：选择文件与语言后执行转写。
   - **麦克风**：选择设备与语言，可选择「录音转写」或「实时转写」，并操作对应开始/停止按钮。

---

## 关闭与退出

建议按以下顺序终止服务，避免未完成请求被中断。

| 顺序 | 操作 |
|------|------|
| 1 | 在浏览器中关闭 `client.html` 页面；若正在录音或实时转写，先点击停止。 |
| 2 | 在运行 `python -m http.server 8080` 的终端中按 **Ctrl+C**，终止前端服务。 |
| 3 | 在运行 `python api_server.py` 的容器内终端中按 **Ctrl+C** 终止 API；输入 `exit` 退出容器。 |
| 4 | （可选）在宿主机执行 `docker stop qwen3-asr-container` 停止容器；或通过 Docker Desktop 退出 Docker。 |

---

## 离线模型部署

当容器内无法访问 Hugging Face（或镜像站仍超时）时，可在宿主机或其它可联网环境预先下载模型，再通过卷挂载与环境变量供容器使用。

**1. 宿主机下载模型**

在仓库根目录（或任意目标目录）下执行，将模型下载到 `./models/Qwen3-ASR-1.7B`：

```cmd
cd D:\Code\GitHub\Qwen3-ASR
set HF_ENDPOINT=https://hf-mirror.com
pip install huggingface_hub
huggingface-cli download Qwen/Qwen3-ASR-1.7B --local-dir ./models/Qwen3-ASR-1.7B
```

或使用 Python：

```python
from huggingface_hub import snapshot_download
snapshot_download("Qwen/Qwen3-ASR-1.7B", local_dir="./models/Qwen3-ASR-1.7B")
```

**2. 创建容器时挂载模型目录**

在 `docker run` 中增加对本地模型目录的挂载（与 [首次部署](#首次部署创建并启动容器) 参数合并使用）：

```cmd
docker run --name qwen3-asr-container -p 8000:80 -it ^
  -v D:\Code\GitHub\Qwen3-ASR\workspace:/data/shared/Qwen3-ASR ^
  -v D:\Code\GitHub\Qwen3-ASR\models\Qwen3-ASR-1.7B:/data/shared/Qwen3-ASR-1.7B ^
  --shm-size=4gb qwenllm/qwen3-asr:latest
```

**3. 容器内指定本地模型并启动 API**

```bash
cd /data/shared/Qwen3-ASR
export QWEN3_ASR_MODEL=/data/shared/Qwen3-ASR-1.7B
python api_server.py
```

---

## 故障排除

| 现象 | 处理建议 |
|------|----------|
| 内存不足 | 增大 `docker run` 的 `--shm-size`（例如 `--shm-size=4gb` 或更高）。 |
| GPU 不可用 | 确认 NVIDIA 驱动与 Docker Desktop 中 “Use NVIDIA Container Runtime” 已启用；GPU 运行需在 `docker run` 中加入 `--gpus all`。 |
| 端口冲突 | 修改映射为其他宿主机端口（如 `-p 8001:80`），并将前端配置中的 API 地址改为 `http://localhost:8001`。 |
| 模型下载失败 | 脚本已默认使用国内镜像；若仍超时，请采用 [离线模型部署](#离线模型部署)。 |
| 无 Docker 时本地运行 | 在宿主机工作目录执行 `pip install qwen-asr flask librosa` 后运行 `python api_server.py`；默认监听 80 端口，若需 8000 端口，可修改 `api_server.py` 中 `port=80` 为 `port=8000`，并保持前端 `API_BASE` 为 `http://localhost:8000`。 |
